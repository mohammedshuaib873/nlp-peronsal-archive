
In the code below, text.txt is the original input file in which stopwords 
are to be removed. filteredtext.txt is the output file. It can be done 
using following code:

import io
from nltk.corpus import stopwords 
from nltk.tokenize import word_tokenize 
#word_tokenize accepts a string as an input, not a file. 
stop_words = set(stopwords.words('english')) 
file1 = open("text.txt") 
line = file1.read()# Use this to read file content as a stream: 
words = line.split() 
for r in words: 
	if not r in stop_words: 
		appendFile = open('filteredtext.txt','a') 
		appendFile.write(" "+r) 
		appendFile.close() 

